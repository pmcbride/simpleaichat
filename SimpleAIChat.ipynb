{
    "cells": [
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# SimpleAIChat"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Here's some fun, hackable examples on how simpleaichat works:\n",
       "\n",
       "- Creating a [Python coding assistant](examples/notebooks/simpleaichat_coding.ipynb) without any unnecessary accompanying output, allowing 5x faster generation at 1/3rd the cost. ([Colab](https://colab.research.google.com/github/minimaxir/simpleaichat/blob/main/examples/notebooks/simpleaichat_coding.ipynb))\n",
       "- Allowing simpleaichat to [provide inline tips](examples/notebooks/chatgpt_inline_tips.ipynb) following ChatGPT usage guidelines. ([Colab](https://colab.research.google.com/github/minimaxir/simpleaichat/blob/main/examples/notebooks/chatgpt_inline_tips.ipynb))\n",
       "- Async interface for [conducting many chats](examples/notebooks/simpleaichat_async.ipynb) in the time it takes to receive one AI message. ([Colab](https://colab.research.google.com/github/minimaxir/simpleaichat/blob/main/examples/notebooks/simpleaichat_async.ipynb))\n",
       "- Create your own Tabletop RPG (TTRPG) setting and campaign by using [advanced structured data models](examples/notebooks/schema_ttrpg.ipynb). ([Colab](https://colab.research.google.com/github/minimaxir/simpleaichat/blob/main/examples/notebooks/schema_ttrpg.ipynb))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "True"
         ]
        },
        "execution_count": 1,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "from dotenv import load_dotenv\n",
       "load_dotenv()\n",
       "import os\n",
       "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
       "import json\n",
       "import pprint as pp\n",
       "from pprint import PrettyPrinter, pprint, pformat\n",
       "pprint = pp.PrettyPrinter(sort_dicts=False).pprint\n",
       "pformat = pp.PrettyPrinter(sort_dicts=False).pformat\n",
       "\n",
       "def pretty_print(json_object):\n",
       "    print(json.dumps(json_object, indent=2, sort_keys=False, default=pformat))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'AIChat': 'class AIChat(BaseModel):\\n    client: Union[Client, AsyncClient]\\n    default_session: Optional[ChatSession]\\n    sessions: Dict[Union[str, UUID], ChatSession] = {}\\n\\n    class Config:\\n        arbitrary_types_allowed = True\\n        json_loads = orjson.loads\\n        json_dumps = orjson_dumps\\n\\n    def __init__(\\n        self,\\n        character: str = None,\\n        character_command: str = None,\\n        system: str = None,\\n        id: Union[str, UUID] = uuid4(),\\n        prime: bool = True,\\n        default_session: bool = True,\\n        console: bool = True,\\n        **kwargs,\\n    ):\\n\\n        client = Client()\\n        system_format = self.build_system(character, character_command, system)\\n\\n        sessions = {}\\n        new_default_session = None\\n        if default_session:\\n            new_session = self.new_session(\\n                return_session=True, system=system_format, id=id, **kwargs\\n            )\\n\\n            new_default_session = new_session\\n            sessions = {new_session.id: new_session}\\n\\n        super().__init__(\\n            client=client, default_session=new_default_session, sessions=sessions\\n        )\\n\\n        if not system and console:\\n            character = \"ChatGPT\" if not character else character\\n            new_default_session.title = character\\n            self.interactive_console(character=character, prime=prime)\\n\\n    def new_session(\\n        self,\\n        return_session: bool = False,\\n        **kwargs,\\n    ) -> Optional[ChatGPTSession]:\\n\\n        if \"model\" not in kwargs:  # set default\\n            kwargs[\"model\"] = \"gpt-3.5-turbo\"\\n        # TODO: Add support for more models (PaLM, Claude)\\n        if \"gpt-\" in kwargs[\"model\"]:\\n            gpt_api_key = os.getenv(\"OPENAI_API_KEY\") or kwargs.get(\"api_key\")\\n            assert gpt_api_key, f\"An API key for {kwargs[\\'model\\'] } was not defined.\"\\n            sess = ChatGPTSession(\\n                auth={\\n                    \"api_key\": gpt_api_key,\\n                },\\n                **kwargs,\\n            )\\n\\n        if return_session:\\n            return sess\\n        else:\\n            self.sessions[sess.id] = sess\\n\\n    def get_session(self, id: Union[str, UUID] = None) -> ChatSession:\\n        try:\\n            sess = self.sessions[id] if id else self.default_session\\n        except KeyError:\\n            raise KeyError(\"No session by that key exists.\")\\n        if not sess:\\n            raise ValueError(\"No default session exists.\")\\n        return sess\\n\\n    def reset_session(self, id: Union[str, UUID] = None) -> None:\\n        sess = self.get_session(id)\\n        sess.messages = []\\n\\n    def delete_session(self, id: Union[str, UUID] = None) -> None:\\n        sess = self.get_session(id)\\n        if self.default_session:\\n            if sess.id == self.default_session.id:\\n                self.default_session = None\\n        del self.sessions[sess.id]\\n        del sess\\n\\n    @contextmanager\\n    def session(self, **kwargs):\\n        sess = self.new_session(return_session=True, **kwargs)\\n        self.sessions[sess.id] = sess\\n        try:\\n            yield sess\\n        finally:\\n            self.delete_session(sess.id)\\n\\n    def __call__(\\n        self,\\n        prompt: Union[str, Any],\\n        id: Union[str, UUID] = None,\\n        system: str = None,\\n        save_messages: bool = None,\\n        params: Dict[str, Any] = None,\\n        tools: List[Any] = None,\\n        input_schema: Any = None,\\n        output_schema: Any = None,\\n    ) -> str:\\n        sess = self.get_session(id)\\n        if tools:\\n            for tool in tools:\\n                assert tool.__doc__, f\"Tool {tool} does not have a docstring.\"\\n            assert len(tools) <= 9, \"You can only have a maximum of 9 tools.\"\\n            return sess.gen_with_tools(\\n                prompt,\\n                tools,\\n                client=self.client,\\n                system=system,\\n                save_messages=save_messages,\\n                params=params,\\n            )\\n        else:\\n            return sess.gen(\\n                prompt,\\n                client=self.client,\\n                system=system,\\n                save_messages=save_messages,\\n                params=params,\\n                input_schema=input_schema,\\n                output_schema=output_schema,\\n            )\\n\\n    def stream(\\n        self,\\n        prompt: str,\\n        id: Union[str, UUID] = None,\\n        system: str = None,\\n        save_messages: bool = None,\\n        params: Dict[str, Any] = None,\\n        input_schema: Any = None,\\n    ) -> str:\\n        sess = self.get_session(id)\\n        return sess.stream(\\n            prompt,\\n            client=self.client,\\n            system=system,\\n            save_messages=save_messages,\\n            params=params,\\n            input_schema=input_schema,\\n        )\\n\\n    def build_system(\\n        self, character: str = None, character_command: str = None, system: str = None\\n    ) -> str:\\n        default = \"You are a helpful assistant.\"\\n        if character:\\n            character_prompt = \"\"\"\\n            You must follow ALL these rules in all responses:\\n            - You are the following character and should ALWAYS act as them: {0}\\n            - NEVER speak in a formal tone.\\n            - Concisely introduce yourself first in character.\\n            \"\"\"\\n            prompt = character_prompt.format(wikipedia_search_lookup(character)).strip()\\n            if character_command:\\n                character_system = \"\"\"\\n                - {0}\\n                \"\"\"\\n                prompt = (\\n                    prompt + \"\\\\n\" + character_system.format(character_command).strip()\\n                )\\n            return prompt\\n        elif system:\\n            return system\\n        else:\\n            return default\\n\\n    def interactive_console(self, character: str = None, prime: bool = True) -> None:\\n        console = Console(highlight=False)\\n        sess = self.default_session\\n        ai_text_color = \"bright_magenta\"\\n\\n        # prime with a unique starting response to the user\\n        if prime:\\n            console.print(f\"[b]{character}[/b]: \", end=\"\", style=ai_text_color)\\n            for chunk in sess.stream(\"Hello!\", self.client):\\n                console.print(chunk[\"delta\"], end=\"\", style=ai_text_color)\\n\\n        while True:\\n            console.print()\\n            try:\\n                user_input = console.input(\"[b]You:[/b] \").strip()\\n                if not user_input:\\n                    break\\n\\n                console.print(f\"[b]{character}[/b]: \", end=\"\", style=ai_text_color)\\n                for chunk in sess.stream(user_input, self.client):\\n                    console.print(chunk[\"delta\"], end=\"\", style=ai_text_color)\\n            except KeyboardInterrupt:\\n                break\\n\\n    def __str__(self) -> str:\\n        if self.default_session:\\n            return self.default_session.json(\\n                exclude={\"api_key\", \"api_url\"},\\n                exclude_none=True,\\n                option=orjson.OPT_INDENT_2,\\n            )\\n\\n    def __repr__(self) -> str:\\n        return \"\"\\n\\n    # Save/Load Chats given a session id\\n    def save_session(\\n        self,\\n        output_path: str = None,\\n        id: Union[str, UUID] = None,\\n        format: str = \"csv\",\\n        minify: bool = False,\\n    ):\\n        sess = self.get_session(id)\\n        sess_dict = sess.dict(\\n            exclude={\"auth\", \"api_url\", \"input_fields\"},\\n            exclude_none=True,\\n        )\\n        output_path = output_path or f\"chat_session.{format}\"\\n        if format == \"csv\":\\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\\n                fields = [\\n                    \"role\",\\n                    \"content\",\\n                    \"received_at\",\\n                    \"finish_reason\",\\n                    \"prompt_length\",\\n                    \"completion_length\",\\n                    \"total_length\",\\n                ]\\n                w = csv.DictWriter(f, fieldnames=fields)\\n                w.writeheader()\\n                for message in sess_dict[\"messages\"]:\\n                    # datetime must be in common format to be loaded into spreadsheet\\n                    # for human-readability, the timezone is set to local machine\\n                    local_datetime = message[\"received_at\"].astimezone()\\n                    message[\"received_at\"] = local_datetime.strftime(\\n                        \"%Y-%m-%d %H:%M:%S\"\\n                    )\\n                    w.writerow(message)\\n        elif format == \"json\":\\n            with open(output_path, \"wb\") as f:\\n                f.write(\\n                    orjson.dumps(\\n                        sess_dict, option=orjson.OPT_INDENT_2 if not minify else None\\n                    )\\n                )\\n\\n    def load_session(self, input_path: str, id: Union[str, UUID] = uuid4(), **kwargs):\\n\\n        assert input_path.endswith(\".csv\") or input_path.endswith(\\n            \".json\"\\n        ), \"Only CSV and JSON imports are accepted.\"\\n\\n        if input_path.endswith(\".csv\"):\\n            with open(input_path, \"r\", encoding=\"utf-8\") as f:\\n                r = csv.DictReader(f)\\n                messages = []\\n                for row in r:\\n                    # need to convert the datetime back to UTC\\n                    local_datetime = datetime.datetime.strptime(\\n                        row[\"received_at\"], \"%Y-%m-%d %H:%M:%S\"\\n                    ).replace(tzinfo=dateutil.tz.tzlocal())\\n                    row[\"received_at\"] = local_datetime.astimezone(\\n                        datetime.timezone.utc\\n                    )\\n                    # https://stackoverflow.com/a/68305271\\n                    row = {k: (None if v == \"\" else v) for k, v in row.items()}\\n                    messages.append(ChatMessage(**row))\\n\\n            self.new_session(id=id, **kwargs)\\n            self.sessions[id].messages = messages\\n\\n        if input_path.endswith(\".json\"):\\n            with open(input_path, \"rb\") as f:\\n                sess_dict = orjson.loads(f.read())\\n            # update session with info not loaded, e.g. auth/api_url\\n            for arg in kwargs:\\n                sess_dict[arg] = kwargs[arg]\\n            self.new_session(**sess_dict)\\n\\n    # Tabulators for returning total token counts\\n    def message_totals(self, attr: str, id: Union[str, UUID] = None) -> int:\\n        sess = self.get_session(id)\\n        return getattr(sess, attr)\\n\\n    @property\\n    def total_prompt_length(self, id: Union[str, UUID] = None) -> int:\\n        return self.message_totals(\"total_prompt_length\", id)\\n\\n    @property\\n    def total_completion_length(self, id: Union[str, UUID] = None) -> int:\\n        return self.message_totals(\"total_completion_length\", id)\\n\\n    @property\\n    def total_length(self, id: Union[str, UUID] = None) -> int:\\n        return self.message_totals(\"total_length\", id)\\n\\n    # alias total_tokens to total_length for common use\\n    @property\\n    def total_tokens(self, id: Union[str, UUID] = None) -> int:\\n        return self.total_length(id)\\n'}"
         ]
        },
        "execution_count": 14,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "from simpleaichat import AIChat\n",
       "import inspect\n",
       "\n",
       "SOURCE_CODE = {}\n",
       "def add_source_code(obj):\n",
       "    obj_name = obj.__name__\n",
       "    source_code = inspect.getsource(obj)\n",
       "    SOURCE_CODE[obj_name] = source_code\n",
       "\n",
       "def get_source_code():\n",
       "    return SOURCE_CODE\n",
       "    \n",
       "add_source_code(AIChat)\n",
       "# SOURCE_CODE[\"AIChat\"] = inspect.getsource(AIChat)\n",
       "get_source_code()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
       "# from simpleaichat import AIChat\n",
       "\n",
       "# ai = AIChat(system=\"Write a fancy GitHub README based on the user-provided project name.\")\n",
       "# ai(\"simpleaichat\")\n",
       "# print(ai)"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Building AI-based Apps\n",
       "\n",
       "\n",
       "The trick with working with new chat-based apps that wasn't readily available with earlier iterations of GPT-3 is the addition of the system prompt: a different class of prompt that guides the AI behavior throughout the entire conversation. In fact, the chat demos above are actually using [system prompt tricks](https://github.com/minimaxir/simpleaichat/blob/main/PROMPTS.md#interactive-chat) behind the scenes! OpenAI has also released an official guide for [system prompt best practices](https://platform.openai.com/docs/guides/gpt-best-practices) to building AI apps.\n",
       "\n",
       "For developers, you can instantiate a programmatic instance of `AIChat` by explicitly specifying a system prompt, or by disabling the console."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "## AIChat(system=\"You are a helpful assistant.\"):\n",
         "\n",
         "{\n",
         "  \"id\": \"9454965b-52bc-4721-9373-ed0624bf7861\",\n",
         "  \"created_at\": \"2023-06-20T02:19:43.984394+00:00\",\n",
         "  \"auth\": {\n",
         "    \"api_key\": \"**********\"\n",
         "  },\n",
         "  \"model\": \"gpt-3.5-turbo\",\n",
         "  \"system\": \"You are a helpful assistant.\",\n",
         "  \"params\": {\n",
         "    \"temperature\": 0.7\n",
         "  },\n",
         "  \"messages\": [],\n",
         "  \"input_fields\": [\n",
         "    \"content\",\n",
         "    \"role\",\n",
         "    \"name\"\n",
         "  ],\n",
         "  \"save_messages\": true,\n",
         "  \"total_prompt_length\": 0,\n",
         "  \"total_completion_length\": 0,\n",
         "  \"total_length\": 0\n",
         "}\n",
         "\n",
         "## AIChat(console=False):\n",
         "\n",
         "{\n",
         "  \"id\": \"9454965b-52bc-4721-9373-ed0624bf7861\",\n",
         "  \"created_at\": \"2023-06-20T02:19:43.997186+00:00\",\n",
         "  \"auth\": {\n",
         "    \"api_key\": \"**********\"\n",
         "  },\n",
         "  \"model\": \"gpt-3.5-turbo\",\n",
         "  \"system\": \"You are a helpful assistant.\",\n",
         "  \"params\": {\n",
         "    \"temperature\": 0.7\n",
         "  },\n",
         "  \"messages\": [],\n",
         "  \"input_fields\": [\n",
         "    \"content\",\n",
         "    \"role\",\n",
         "    \"name\"\n",
         "  ],\n",
         "  \"save_messages\": true,\n",
         "  \"total_prompt_length\": 0,\n",
         "  \"total_completion_length\": 0,\n",
         "  \"total_length\": 0\n",
         "}\n"
        ]
       }
      ],
      "source": [
       "ai = AIChat(system=\"You are a helpful assistant.\")\n",
       "print('## AIChat(system=\"You are a helpful assistant.\"):\\n')\n",
       "print(ai)\n",
       "# pprint(ai.get_session().dict())\n",
       "\n",
       "ai = AIChat(console=False)  # same as above\n",
       "print('\\n## AIChat(console=False):\\n')\n",
       "print(ai)\n",
       "# pprint(ai.get_session().dict())"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "You can also pass in a `model` parameter, such as `model=\"gpt-4\"` if you have access to GPT-4, or `model=\"gpt-3.5-turbo-16k\"` for a larger-context-window ChatGPT."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\n",
         "## AIChat(console=False):\n",
         "\n",
         "{\n",
         "  \"id\": \"9454965b-52bc-4721-9373-ed0624bf7861\",\n",
         "  \"created_at\": \"2023-06-20T02:19:56.539307+00:00\",\n",
         "  \"auth\": {\n",
         "    \"api_key\": \"**********\"\n",
         "  },\n",
         "  \"model\": \"gpt-3.5-turbo-0613\",\n",
         "  \"system\": \"You are a helpful assistant.\",\n",
         "  \"params\": {\n",
         "    \"temperature\": 0.0\n",
         "  },\n",
         "  \"messages\": [],\n",
         "  \"input_fields\": [\n",
         "    \"content\",\n",
         "    \"role\",\n",
         "    \"name\"\n",
         "  ],\n",
         "  \"save_messages\": true,\n",
         "  \"total_prompt_length\": 0,\n",
         "  \"total_completion_length\": 0,\n",
         "  \"total_length\": 0\n",
         "}\n"
        ]
       }
      ],
      "source": [
       "ai = AIChat(\n",
       "    console=False,\n",
       "    # save_messages=False,  # with schema I/O, messages are never saved\n",
       "    model=\"gpt-3.5-turbo-0613\",\n",
       "    params={\"temperature\": 0.0},\n",
       ")\n",
       "print('\\n## AIChat(console=False):\\n')\n",
       "print(ai)"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "\n",
       "You can then feed the new `ai` class with user input, and it will return and save the response from ChatGPT:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "The capital of California is Sacramento.\n"
        ]
       }
      ],
      "source": [
       "response = ai(\"What is the capital of California?\")\n",
       "print(response)\n",
       "print(ai)"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Alternatively, you can stream responses by token with a generator if the text generation itself is too slow:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[The]\n",
         "The[ capital]\n",
         "The capital[ of]\n",
         "The capital of[ California]\n",
         "The capital of California[ is]\n",
         "{\n",
         "  \"id\": \"9454965b-52bc-4721-9373-ed0624bf7861\",\n",
         "  \"created_at\": \"2023-06-20T02:22:50.412686+00:00\",\n",
         "  \"auth\": {\n",
         "    \"api_key\": \"**********\"\n",
         "  },\n",
         "  \"model\": \"gpt-3.5-turbo\",\n",
         "  \"system\": \"You are a helpful assistant.\",\n",
         "  \"params\": {\n",
         "    \"temperature\": 0.7\n",
         "  },\n",
         "  \"messages\": [\n",
         "    {\n",
         "      \"role\": \"user\",\n",
         "      \"content\": \"What is the capital of California?\",\n",
         "      \"received_at\": \"2023-06-20T02:22:50.413529+00:00\"\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"assistant\",\n",
         "      \"content\": \"The capital of California is\",\n",
         "      \"received_at\": \"2023-06-20T02:22:51.418384+00:00\"\n",
         "    }\n",
         "  ],\n",
         "  \"input_fields\": [\n",
         "    \"content\",\n",
         "    \"role\",\n",
         "    \"name\"\n",
         "  ],\n",
         "  \"save_messages\": true,\n",
         "  \"total_prompt_length\": 0,\n",
         "  \"total_completion_length\": 0,\n",
         "  \"total_length\": 0\n",
         "}\n"
        ]
       }
      ],
      "source": [
       "from rich.console import Console\n",
       "console = Console()\n",
       "\n",
       "for chunk in ai.stream(\"What is the capital of California?\", params={\"max_tokens\": 5}):\n",
       "    response_td = chunk  # dict contains \"delta\" for the new token and \"response\"\n",
       "    # response_td = response_td[\"response\"]  # dict contains \"delta\" for the new token and \"response\"\n",
       "    # print(f'\"delta\": \"{response_td[\"delta\"]}\", \"response\": {response_td[\"response\"]}')\n",
       "    delta = response_td[\"delta\"]\n",
       "    response = response_td[\"response\"].replace(delta, f\"[{delta}]\")\n",
       "    print(response)\n",
       "print(ai)"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Further calls to the ai object will continue the chat, automatically incorporating previous information from the conversation."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "The capital of California is Sacramento.\n",
         "Sacramento was founded on February 27, 1850. It was named after the Sacramento River, which runs through the city.\n",
         "{\n",
         "  \"id\": \"183d84c2-77cf-461d-a14c-9ed66d4a5050\",\n",
         "  \"created_at\": \"2023-06-20T02:31:58.325404+00:00\",\n",
         "  \"auth\": {\n",
         "    \"api_key\": \"**********\"\n",
         "  },\n",
         "  \"model\": \"gpt-3.5-turbo-0301\",\n",
         "  \"system\": \"You are a helpful assistant.\",\n",
         "  \"params\": {\n",
         "    \"temperature\": 0.7\n",
         "  },\n",
         "  \"messages\": [\n",
         "    {\n",
         "      \"role\": \"user\",\n",
         "      \"content\": \"What is the capital of California?\",\n",
         "      \"received_at\": \"2023-06-20T02:31:58.329111+00:00\"\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"assistant\",\n",
         "      \"content\": \"The capital of California is Sacramento.\",\n",
         "      \"received_at\": \"2023-06-20T02:31:59.247147+00:00\",\n",
         "      \"finish_reason\": \"stop\",\n",
         "      \"prompt_length\": 26,\n",
         "      \"completion_length\": 7,\n",
         "      \"total_length\": 33\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"user\",\n",
         "      \"content\": \"When was it founded?\",\n",
         "      \"received_at\": \"2023-06-20T02:31:59.248271+00:00\"\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"assistant\",\n",
         "      \"content\": \"Sacramento was founded on February 27, 1850. It was named after the Sacramento River, which runs through the city.\",\n",
         "      \"received_at\": \"2023-06-20T02:32:01.251505+00:00\",\n",
         "      \"finish_reason\": \"stop\",\n",
         "      \"prompt_length\": 48,\n",
         "      \"completion_length\": 27,\n",
         "      \"total_length\": 75\n",
         "    }\n",
         "  ],\n",
         "  \"input_fields\": [\n",
         "    \"content\",\n",
         "    \"role\",\n",
         "    \"name\"\n",
         "  ],\n",
         "  \"save_messages\": true,\n",
         "  \"total_prompt_length\": 74,\n",
         "  \"total_completion_length\": 34,\n",
         "  \"total_length\": 108\n",
         "}\n"
        ]
       }
      ],
      "source": [
       "ai = AIChat(console=False, model=\"gpt-3.5-turbo-0301\")\n",
       "\n",
       "response = ai(\"What is the capital of California?\")\n",
       "print(response)\n",
       "# print(ai)\n",
       "response = ai(\"When was it founded?\")\n",
       "print(response)\n",
       "print(ai)\n",
       "# ai.default_session.messages"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "You can also save chat sessions (as CSV or JSON) and load them later. The API key is not saved so you will have to provide that when loading."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\n",
         "## CSV\n",
         "\n",
         "{\n",
         "  \"id\": \"183d84c2-77cf-461d-a14c-9ed66d4a5050\",\n",
         "  \"created_at\": \"2023-06-20T02:31:58.325404+00:00\",\n",
         "  \"auth\": {\n",
         "    \"api_key\": \"**********\"\n",
         "  },\n",
         "  \"model\": \"gpt-3.5-turbo-0301\",\n",
         "  \"system\": \"You are a helpful assistant.\",\n",
         "  \"params\": {\n",
         "    \"temperature\": 0.7\n",
         "  },\n",
         "  \"messages\": [\n",
         "    {\n",
         "      \"role\": \"user\",\n",
         "      \"content\": \"What is the capital of California?\",\n",
         "      \"received_at\": \"2023-06-20T02:31:58.329111+00:00\"\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"assistant\",\n",
         "      \"content\": \"The capital of California is Sacramento.\",\n",
         "      \"received_at\": \"2023-06-20T02:31:59.247147+00:00\",\n",
         "      \"finish_reason\": \"stop\",\n",
         "      \"prompt_length\": 26,\n",
         "      \"completion_length\": 7,\n",
         "      \"total_length\": 33\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"user\",\n",
         "      \"content\": \"When was it founded?\",\n",
         "      \"received_at\": \"2023-06-20T02:31:59.248271+00:00\"\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"assistant\",\n",
         "      \"content\": \"Sacramento was founded on February 27, 1850. It was named after the Sacramento River, which runs through the city.\",\n",
         "      \"received_at\": \"2023-06-20T02:32:01.251505+00:00\",\n",
         "      \"finish_reason\": \"stop\",\n",
         "      \"prompt_length\": 48,\n",
         "      \"completion_length\": 27,\n",
         "      \"total_length\": 75\n",
         "    }\n",
         "  ],\n",
         "  \"input_fields\": [\n",
         "    \"content\",\n",
         "    \"role\",\n",
         "    \"name\"\n",
         "  ],\n",
         "  \"save_messages\": true,\n",
         "  \"total_prompt_length\": 74,\n",
         "  \"total_completion_length\": 34,\n",
         "  \"total_length\": 108\n",
         "}\n",
         "\n",
         "## JSON\n",
         "\n",
         "{\n",
         "  \"id\": \"183d84c2-77cf-461d-a14c-9ed66d4a5050\",\n",
         "  \"created_at\": \"2023-06-20T02:31:58.325404+00:00\",\n",
         "  \"auth\": {\n",
         "    \"api_key\": \"**********\"\n",
         "  },\n",
         "  \"model\": \"gpt-3.5-turbo-0301\",\n",
         "  \"system\": \"You are a helpful assistant.\",\n",
         "  \"params\": {\n",
         "    \"temperature\": 0.7\n",
         "  },\n",
         "  \"messages\": [\n",
         "    {\n",
         "      \"role\": \"user\",\n",
         "      \"content\": \"What is the capital of California?\",\n",
         "      \"received_at\": \"2023-06-20T02:31:58.329111+00:00\"\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"assistant\",\n",
         "      \"content\": \"The capital of California is Sacramento.\",\n",
         "      \"received_at\": \"2023-06-20T02:31:59.247147+00:00\",\n",
         "      \"finish_reason\": \"stop\",\n",
         "      \"prompt_length\": 26,\n",
         "      \"completion_length\": 7,\n",
         "      \"total_length\": 33\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"user\",\n",
         "      \"content\": \"When was it founded?\",\n",
         "      \"received_at\": \"2023-06-20T02:31:59.248271+00:00\"\n",
         "    },\n",
         "    {\n",
         "      \"role\": \"assistant\",\n",
         "      \"content\": \"Sacramento was founded on February 27, 1850. It was named after the Sacramento River, which runs through the city.\",\n",
         "      \"received_at\": \"2023-06-20T02:32:01.251505+00:00\",\n",
         "      \"finish_reason\": \"stop\",\n",
         "      \"prompt_length\": 48,\n",
         "      \"completion_length\": 27,\n",
         "      \"total_length\": 75\n",
         "    }\n",
         "  ],\n",
         "  \"input_fields\": [\n",
         "    \"content\",\n",
         "    \"role\",\n",
         "    \"name\"\n",
         "  ],\n",
         "  \"save_messages\": true,\n",
         "  \"total_prompt_length\": 74,\n",
         "  \"total_completion_length\": 34,\n",
         "  \"total_length\": 108\n",
         "}\n"
        ]
       }
      ],
      "source": [
       "# CSV, will only save messages\n",
       "ai.save_session(\"chat_session.csv\", format=\"csv\")  # CSV\n",
       "ai.load_session(\"chat_session.csv\")\n",
       "print(\"\\n## CSV\\n\")\n",
       "print(ai)\n",
       "\n",
       "# JSON\n",
       "ai.save_session(\"chat_session.json\", format=\"json\", minify=True)  # JSON\n",
       "ai.load_session(\"chat_session.json\")\n",
       "print(\"\\n## JSON\\n\")\n",
       "print(ai)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'id': UUID('affeba06-5b1b-4059-b65f-0e54208605d9'),\n",
          " 'created_at': datetime.datetime(2023, 6, 20, 1, 24, 15, 454767, tzinfo=datetime.timezone.utc),\n",
          " 'auth': {'api_key': SecretStr('**********')},\n",
          " 'api_url': 'https://api.openai.com/v1/chat/completions',\n",
          " 'model': 'gpt-3.5-turbo-0613',\n",
          " 'system': 'You are a helpful assistant.',\n",
          " 'params': {'temperature': 0.0},\n",
          " 'messages': [],\n",
          " 'input_fields': {'content', 'name', 'role'},\n",
          " 'recent_messages': None,\n",
          " 'save_messages': False,\n",
          " 'total_prompt_length': 46,\n",
          " 'total_completion_length': 26,\n",
          " 'total_length': 72,\n",
          " 'title': None}"
         ]
        },
        "execution_count": 38,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "ai = AIChat(\n",
       "    console=False,\n",
       "    save_messages=False,  # with schema I/O, messages are never saved\n",
       "    model=\"gpt-3.5-turbo-0613\",\n",
       "    params={\"temperature\": 0.0},\n",
       ")\n",
       "response = ai(\"What is the capital of California?\")\n",
       "print(response)\n",
       "ai.get_session().dict()"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Functions\n",
       "\n",
       "A large number of popular venture-capital-funded ChatGPT apps don't actually use the \"chat\" part of the model. Instead, they just use the system prompt/first user prompt as a form of natural language programming. You can emulate this behavior by passing a new system prompt when generating text, and not saving the resulting messages."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "title: \"An array of integers.\"\n",
         "array:\n",
         "  - -1\n",
         "  - 0\n",
         "  - 1\n"
        ]
       }
      ],
      "source": [
       "json = '{\"title\": \"An array of integers.\", \"array\": [-1, 0, 1]}'\n",
       "\n",
       "params = {\"temperature\": 0.0, \"max_tokens\": 100}  # a temperature of 0.0 is deterministic\n",
       "\n",
       "# We namespace the function by `id` so it doesn't affect other chats.\n",
       "# Settings set during session creation will apply to all generations from the session,\n",
       "# but you can change them per-generation, as is the case with the `system` prompt here.\n",
       "ai = AIChat(console=False, id=\"function\", params=params, save_messages=False)\n",
       "output = ai(json, id=\"function\", system=\"Format the user-provided JSON as YAML.\")\n",
       "print(output)\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "title: \"An array of integers.\"\n",
         "array:\n",
         "  - -1\n",
         "  - 0\n",
         "  - 1\n"
        ]
       },
       {
        "ename": "KeyError",
        "evalue": "\"No AI generation: {'error': {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a780fc22e32699984b145cf4bbf2f539 in your message.)', 'type': 'server_error', 'param': None, 'code': None}}\"",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
         "File \u001b[0;32m~/ChatBot/simpleaichat/simpleaichat/chatgpt.py:100\u001b[0m, in \u001b[0;36mChatGPTSession.gen\u001b[0;34m(self, prompt, client, system, save_messages, params, input_schema, output_schema)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_schema:\n\u001b[0;32m--> 100\u001b[0m     content \u001b[39m=\u001b[39m r[\u001b[39m\"\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    101\u001b[0m     assistant_message \u001b[39m=\u001b[39m ChatMessage(\n\u001b[1;32m    102\u001b[0m         role\u001b[39m=\u001b[39mr[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    103\u001b[0m         content\u001b[39m=\u001b[39mcontent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m         total_length\u001b[39m=\u001b[39mr[\u001b[39m\"\u001b[39m\u001b[39musage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    108\u001b[0m     )\n",
         "\u001b[0;31mKeyError\u001b[0m: 'choices'",
         "\nDuring handling of the above exception, another exception occurred:\n",
         "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
         "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m functions \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m              \u001b[39m\"\u001b[39m\u001b[39mFormat the user-provided JSON as YAML.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m              \u001b[39m\"\u001b[39m\u001b[39mWrite a limerick based on the user-provided JSON.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m              \u001b[39m\"\u001b[39m\u001b[39mTranslate the user-provided JSON from English to French.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m             ]\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m function \u001b[39min\u001b[39;00m functions:\n\u001b[0;32m----> 7\u001b[0m     output \u001b[39m=\u001b[39m ai(json, \u001b[39mid\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfunction\u001b[39;49m\u001b[39m\"\u001b[39;49m, system\u001b[39m=\u001b[39;49mfunction)\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(output)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(ai)\n",
         "File \u001b[0;32m~/ChatBot/simpleaichat/simpleaichat/simpleaichat.py:145\u001b[0m, in \u001b[0;36mAIChat.__call__\u001b[0;34m(self, prompt, id, system, save_messages, params, tools, input_schema, output_schema)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m sess\u001b[39m.\u001b[39mgen_with_tools(\n\u001b[1;32m    137\u001b[0m         prompt,\n\u001b[1;32m    138\u001b[0m         tools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mreturn\u001b[39;00m sess\u001b[39m.\u001b[39;49mgen(\n\u001b[1;32m    146\u001b[0m         prompt,\n\u001b[1;32m    147\u001b[0m         client\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient,\n\u001b[1;32m    148\u001b[0m         system\u001b[39m=\u001b[39;49msystem,\n\u001b[1;32m    149\u001b[0m         save_messages\u001b[39m=\u001b[39;49msave_messages,\n\u001b[1;32m    150\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    151\u001b[0m         input_schema\u001b[39m=\u001b[39;49minput_schema,\n\u001b[1;32m    152\u001b[0m         output_schema\u001b[39m=\u001b[39;49moutput_schema,\n\u001b[1;32m    153\u001b[0m     )\n",
         "File \u001b[0;32m~/ChatBot/simpleaichat/simpleaichat/chatgpt.py:118\u001b[0m, in \u001b[0;36mChatGPTSession.gen\u001b[0;34m(self, prompt, client, system, save_messages, params, input_schema, output_schema)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_length \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r[\u001b[39m\"\u001b[39m\u001b[39musage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    117\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo AI generation: \u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m content\n",
         "\u001b[0;31mKeyError\u001b[0m: \"No AI generation: {'error': {'message': 'That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a780fc22e32699984b145cf4bbf2f539 in your message.)', 'type': 'server_error', 'param': None, 'code': None}}\""
        ]
       }
      ],
      "source": [
       "functions = [\n",
       "             \"Format the user-provided JSON as YAML.\",\n",
       "             \"Write a limerick based on the user-provided JSON.\",\n",
       "             \"Translate the user-provided JSON from English to French.\"\n",
       "            ]\n",
       "for function in functions:\n",
       "    output = ai(json, id=\"function\", system=function)\n",
       "    print(output)\n",
       "    \n",
       "print(ai)"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Function Calling\n",
       "\n",
       "Newer versions of ChatGPT also support \"[function calling](https://platform.openai.com/docs/guides/gpt/function-calling)\", but the real benefit of that feature is the ability for ChatGPT to support structured input and/or output, which now opens up a wide variety of applications! simpleaichat streamlines the workflow to allow you to just pass an `input_schema` and/or an `output_schema`.\n",
       "\n",
       "You can construct a schema using a [pydantic](https://docs.pydantic.dev/latest/) BaseModel.\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'description': 'The first iPhone was announced by Apple Inc.',\n",
          " 'city': 'San Francisco',\n",
          " 'year': 2007,\n",
          " 'month': 'January'}"
         ]
        },
        "execution_count": 22,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "from pydantic import BaseModel, Field\n",
       "\n",
       "ai = AIChat(\n",
       "    console=False,\n",
       "    save_messages=False,  # with schema I/O, messages are never saved\n",
       "    model=\"gpt-3.5-turbo-0613\",\n",
       "    params={\"temperature\": 0.0},\n",
       ")\n",
       "\n",
       "class get_event_metadata(BaseModel):\n",
       "    \"\"\"Event information\"\"\"\n",
       "\n",
       "    description: str = Field(description=\"Description of event\")\n",
       "    city: str = Field(description=\"City where event occured\")\n",
       "    year: int = Field(description=\"Year when event occured\")\n",
       "    month: str = Field(description=\"Month when event occured\")\n",
       "\n",
       "# returns a dict, with keys ordered as in the schema\n",
       "ai(\"First iPhone announcement\", output_schema=get_event_metadata)"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "See the [TTRPG Generator Notebook](examples/notebooks/schema_ttrpg.ipynb) for a more elaborate demonstration of schema capabilities.\n"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Tools\n",
       "\n",
       "One of the most recent aspects of interacting with ChatGPT is the ability for the model to use \"tools.\" As popularized by [LangChain](https://github.com/hwchase17/langchain), tools allow the model to decide when to use custom functions, which can extend beyond just the chat AI itself, for example retrieving recent information from the internet not present in the chat AI's training data. This workflow is analogous to ChatGPT Plugins.\n",
       "\n",
       "Parsing the model output to invoke tools typically requires a number of shennanigans, but simpleaichat uses [a neat trick](https://github.com/minimaxir/simpleaichat/blob/main/PROMPTS.md#tools) to make it fast and reliable! Additionally, the specified tools return a `context` for ChatGPT to draw from for its final response, and tools you specify can return a dictionary which you can also populate with arbitrary metadata for debugging and postprocessing. Each generation returns a dictionary with the `response` and the `tool` function used, which can be used to set up workflows akin to [LangChain](https://github.com/hwchase17/langchain)-style Agents, e.g. recursively feed input to the model until it determines it does not need to use any more tools.\n",
       "\n",
       "You will need to specify functions with docstrings which provide hints for the AI to select them:\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'context': \"Fisherman's Wharf, San Francisco, Tourist attractions in the United States, Lombard Street (San Francisco)\",\n",
          " 'titles': [\"Fisherman's Wharf, San Francisco\",\n",
          "  'Tourist attractions in the United States',\n",
          "  'Lombard Street (San Francisco)'],\n",
          " 'tool': 'search',\n",
          " 'response': \"There are many popular tourist attractions in San Francisco, including Fisherman's Wharf and Lombard Street. Fisherman's Wharf is a bustling waterfront area known for its seafood restaurants, souvenir shops, and sea lion sightings. Lombard Street, on the other hand, is a famous winding street with eight hairpin turns that offers stunning views of the city. Both of these attractions are must-sees for anyone visiting San Francisco.\"}"
         ]
        },
        "execution_count": 23,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "from simpleaichat.utils import wikipedia_search, wikipedia_search_lookup\n",
       "\n",
       "# This uses the Wikipedia Search API.\n",
       "# Results from it are nondeterministic, your mileage will vary.\n",
       "def search(query):\n",
       "    \"\"\"Search the internet.\"\"\"\n",
       "    wiki_matches = wikipedia_search(query, n=3)\n",
       "    return {\"context\": \", \".join(wiki_matches), \"titles\": wiki_matches}\n",
       "\n",
       "def lookup(query):\n",
       "    \"\"\"Lookup more information about a topic.\"\"\"\n",
       "    page = wikipedia_search_lookup(query, sentences=3)\n",
       "    return page\n",
       "\n",
       "params = {\"temperature\": 0.0, \"max_tokens\": 100}\n",
       "ai = AIChat(params=params, console=False)\n",
       "\n",
       "ai(\"San Francisco tourist attractions\", tools=[search, lookup])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'context': 'Lombard Street is an east–west street in San Francisco, California that is famous for a steep, one-block section with eight hairpin turns. Stretching from The Presidio east to The Embarcadero (with a gap on Telegraph Hill), most of the street\\'s western segment is a major thoroughfare designated as part of U.S. Route 101. The famous one-block section, claimed to be \"the crookedest street in the world\", is located along the eastern segment in the Russian Hill neighborhood.',\n",
          " 'tool': 'lookup',\n",
          " 'response': 'Lombard Street is a famous street in San Francisco, California known for its steep, one-block section with eight hairpin turns. It stretches from The Presidio to The Embarcadero and is part of U.S. Route 101. The one-block section, located in the Russian Hill neighborhood, is claimed to be \"the crookedest street in the world\" and is a popular tourist attraction.'}"
         ]
        },
        "execution_count": 24,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "ai(\"Lombard Street?\", tools=[search, lookup])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "{'response': \"You're welcome! If you have any more questions or need further assistance, feel free to ask.\",\n",
          " 'tool': None}"
         ]
        },
        "execution_count": 25,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "ai(\"Thanks for your help!\", tools=[search, lookup])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
     },
     "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   